import SwiftUI // Or UIKit/AppKit depending on your project type
import CoreML
import Accelerate // For potential MLMultiArray manipulation if needed

// --- Model Names and Action Labels ---
// These should match the models you saved and the actions defined in Python
// Assuming holistic.mlmodel is the 8-bit version based on notebook code
let modelNames = ["holistic_16bit", "holistic_8bit", "holistic_4bit", "holistic_2bit", "holistic_1bit"] // Updated model names
let actionLabels = ["hello", "thanks", "iloveyou"] // From your Python notebook

// --- Data Loading ---
// Loads the X_test and y_test data from the CSV generated by the notebook.
struct TestSample {
    let keypoints: MLMultiArray // Input for the model (shape: [1, 30, 1662])
    let trueLabelIndex: Int    // Index of the correct action
}

func loadTestDataFromCSV() -> [TestSample] {
    var testSamples: [TestSample] = []
    let sequenceLength = 30
    let keypointDim = 1662
    let expectedFeatureCount = sequenceLength * keypointDim // 49860
    let expectedShape: [NSNumber] = [1, NSNumber(value: sequenceLength), NSNumber(value: keypointDim)]

    // 1. Find the CSV file in the app bundle
    // IMPORTANT: Make sure 'holistic_test_data.csv' is added to your Xcode project target
    // and included in the 'Copy Bundle Resources' build phase.
    guard let csvURL = Bundle.main.url(forResource: "holistic_test_data", withExtension: "csv") else {
        print("❌ Error: Could not find holistic_test_data.csv in the app bundle.")
        print("   Ensure the file exists and is added to the target's 'Copy Bundle Resources'.")
        return []
    }

    print("Found CSV at: \(csvURL.path)")

    do {
        // 2. Read the CSV content
        let csvContent = try String(contentsOf: csvURL, encoding: .utf8)
        let lines = csvContent.split(whereSeparator: \.isNewline)

        guard lines.count > 1 else { // Check if there's more than just the header
            print("❌ Error: CSV file seems empty or only contains a header.")
            return []
        }

        print("Read \(lines.count - 1) data lines from CSV.")

        // 3. Parse each line (skip header)
        for (lineIndex, line) in lines.dropFirst().enumerated() {
            let columns = line.split(separator: ",").map { String($0) }

            // Check if the number of columns is correct (features + 1 label)
            guard columns.count == expectedFeatureCount + 1 else {
                print("⚠️ Warning: Skipping line \(lineIndex + 1) due to incorrect column count (expected \(expectedFeatureCount + 1), got \(columns.count)).")
                continue
            }

            // 4. Extract features and label
            let featureStrings = columns.dropLast()
            guard let labelString = columns.last, let trueLabel = Int(labelString) else {
                print("⚠️ Warning: Skipping line \(lineIndex + 1) due to invalid label format.")
                continue
            }

            // 5. Convert feature strings to Floats
            let features = featureStrings.compactMap { Float($0) }
            guard features.count == expectedFeatureCount else {
                print("⚠️ Warning: Skipping line \(lineIndex + 1) after conversion. Incorrect number of valid features (expected \(expectedFeatureCount), got \(features.count)).")
                continue
            }

            // 6. Create MLMultiArray and populate it
            guard let multiArray = try? MLMultiArray(shape: expectedShape, dataType: .float32) else {
                print("❌ Error: Could not create MLMultiArray for line \(lineIndex + 1). Skipping sample.")
                // Consider stopping entirely if this happens, as it indicates a fundamental issue.
                continue
                // fatalError("Could not create MLMultiArray") // Or use fatalError if this shouldn't happen
            }

            let pointer = multiArray.dataPointer.bindMemory(to: Float32.self, capacity: multiArray.count)
            // Populate the MLMultiArray directly from the features array
            for i in 0..<features.count {
                pointer[i] = features[i]
            }

            // 7. Create TestSample
            testSamples.append(TestSample(keypoints: multiArray, trueLabelIndex: trueLabel))
        }

    } catch {
        print("❌ Error reading or parsing CSV file: \(error)")
        return []
    }
    print("✅ Successfully loaded \(testSamples.count) test samples from CSV.")
    return testSamples
}

// --- Model Evaluation Class ---

class ModelEvaluator: ObservableObject {
    // Dictionary to store loaded models (optional, for caching)
    var loadedModels: [String: MLModel] = [:]
    // Dictionary to store results: [ModelName: (Accuracy, AvgInferenceTime)]
    @Published var evaluationResults: [String: (Double, Double)] = [:] // Use @Published for SwiftUI updates
    @Published var isEvaluating: Bool = false // Track evaluation state

    // Function to load a specific Core ML model by name
    func loadModel(name: String) -> MLModel? {
        if let cachedModel = loadedModels[name] {
            print("Using cached model: \(name)")
            return cachedModel
        }

        // Ensure the compiled model (.mlmodelc) is included in the app target
        guard let modelURL = Bundle.main.url(forResource: name, withExtension: "mlmodelc") else {
            print("❌ Error: Could not find compiled model \(name).mlmodelc in bundle. Make sure it's added to the target.")
            return nil
        }

        do {
            let model = try MLModel(contentsOf: modelURL)
            print("✅ Successfully loaded model: \(name)")
            loadedModels[name] = model
            return model
        } catch {
            print("❌ Error: Could not load model \(name) from \(modelURL): \(error)")
            return nil
        }
    }

    // Function to run evaluation on all models
    func evaluateAllModels() {
        DispatchQueue.main.async {
            self.isEvaluating = true // Update state on main thread
            self.evaluationResults = [:] // Clear previous results
        }

        // Load data from CSV instead of generating dummy data
        let testData = loadTestDataFromCSV()
        guard !testData.isEmpty else {
            print("❌ Error: Test data from CSV is empty or failed to load. Cannot evaluate.")
            DispatchQueue.main.async {
                self.isEvaluating = false
            }
            return
        }

        print("\n--- Starting Model Evaluation using data from CSV ---")

        for modelName in modelNames {
            print("\nEvaluating model: \(modelName)...")
            guard let model = loadModel(name: modelName) else {
                print("Skipping evaluation for \(modelName) due to loading error.")
                continue // Skip if model loading failed
            }

            // Verify input/output names (crucial!)
            // These names MUST match your model's actual layer names.
            // Inspect the .mlmodel file in Xcode to confirm.
            let inputFeatureName = "lstm_input" // <<< VERIFY THIS NAME IN XCODE
            let outputFeatureName = "Identity"   // <<< VERIFY THIS NAME IN XCODE
            print("  Expecting input name: '\(inputFeatureName)', output name: '\(outputFeatureName)'")

            var totalInferenceTime: Double = 0
            var correctPredictions: Int = 0
            var predictionErrors: Int = 0

            for (index, sample) in testData.enumerated() {
                // 1. Prepare Input
                let inputProvider = MLFeatureProviderWrapper(featureProvider: sample.keypoints, inputName: inputFeatureName)

                // 2. Measure Inference Time & Predict
                let startTime = Date()
                var predictionOutput: MLFeatureProvider?
                do {
                    predictionOutput = try model.prediction(from: inputProvider)
                } catch {
                    print("❌ Prediction error for model \(modelName), sample \(index): \(error)")
                    predictionErrors += 1
                    continue // Skip this sample if prediction fails
                }
                let inferenceTime = Date().timeIntervalSince(startTime)
                totalInferenceTime += inferenceTime

                // 3. Get Prediction Result
                guard let outputValue = predictionOutput?.featureValue(for: outputFeatureName),
                      let outputMultiArray = outputValue.multiArrayValue else {
                    print("❌ Error: Could not get output feature '\(outputFeatureName)' (MLMultiArray) from model \(modelName)")
                    predictionErrors += 1
                    continue
                }

                // Check output shape (should be [1, num_actions] or similar)
                 // print("Output shape: \(outputMultiArray.shape)") // Uncomment to debug shape

                // Find the index with the highest probability
                let predictedIndex = argmax(outputMultiArray)
                guard predictedIndex != -1 else {
                    print("❌ Error: Could not determine prediction from output array for model \(modelName)")
                    predictionErrors += 1
                    continue
                }

                // 4. Compare with True Label
                if predictedIndex == sample.trueLabelIndex {
                    correctPredictions += 1
                }
                 // else { // Uncomment for debugging mismatches
                 //    print("  Mismatch Sample \(index): Predicted \(actionLabels[predictedIndex]) (\(predictedIndex)), True: \(actionLabels[sample.trueLabelIndex]) (\(sample.trueLabelIndex))")
                 // }
            }

            // Calculate metrics
            let evaluatedSamples = testData.count - predictionErrors
            let accuracy = evaluatedSamples > 0 ? Double(correctPredictions) / Double(evaluatedSamples) : 0.0
            let averageInferenceTime = evaluatedSamples > 0 ? totalInferenceTime / Double(evaluatedSamples) : 0.0

            // Store results on main thread for UI updates
            DispatchQueue.main.async {
                self.evaluationResults[modelName] = (accuracy, averageInferenceTime)
            }

            print("  Samples Evaluated: \(evaluatedSamples) (Errors: \(predictionErrors))")
            print("  Correct Predictions: \(correctPredictions)")
            // Accuracy is now based on real test data
            print("  Accuracy: \(String(format: "%.4f", accuracy))")
            print("  Average Inference Time: \(String(format: "%.6f", averageInferenceTime)) seconds")
        }

        print("\n--- Evaluation Complete ---")
        DispatchQueue.main.async {
            self.isEvaluating = false // Update state on main thread
        }
        displayResults() // Optional: Function to log results
    }

    // Helper function to get the index of the maximum value in an MLMultiArray (output probabilities)
    func argmax(_ array: MLMultiArray) -> Int {
        guard array.count > 0 else {
            print("Error in argmax: array is empty.")
            return -1
        }

        // Determine data type (Float32 is common, but could be Float16 or Double)
        let dataType = array.dataType
        var maxVal: Double = -Double.infinity
        var maxIdx: Int = -1

        let count = array.count
        let ptr = array.dataPointer

        switch dataType {
        case .float32:
            let floatPtr = ptr.bindMemory(to: Float32.self, capacity: count)
            for i in 0..<count {
                let value = Double(floatPtr[i])
                if value > maxVal {
                    maxVal = value
                    maxIdx = i
                }
            }
        case .double:
             let doublePtr = ptr.bindMemory(to: Double.self, capacity: count)
             for i in 0..<count {
                 let value = doublePtr[i]
                 if value > maxVal {
                     maxVal = value
                     maxIdx = i
                 }
             }
        case .float16:
             // Requires conversion if using Float16 (common with quantization)
             // This is a simplified conversion; Accelerate framework might be more robust
             let float16Ptr = ptr.bindMemory(to: UInt16.self, capacity: count) // Assuming Float16 is represented as UInt16 bits
             for i in 0..<count {
                 var f16 = float16Ptr[i]
                 var f32: Float32 = 0
                 // Basic bitwise conversion (may need refinement based on actual Float16 format)
                 let sign = (f16 >> 15) & 0x1
                 let exponent = (f16 >> 10) & 0x1F
                 let mantissa = f16 & 0x3FF

                 if exponent == 0 { // Denormalized or zero
                     f32 = Float32(sign: sign == 1 ? .minus : .plus, exponent: -15, significand: Float(mantissa) / 1024.0)
                 } else if exponent == 31 { // Infinity or NaN
                     f32 = (mantissa == 0) ? (sign == 1 ? -Float32.infinity : Float32.infinity) : Float32.nan
                 } else { // Normalized
                     f32 = Float32(sign: sign == 1 ? .minus : .plus, exponent: Int(exponent) - 15, significand: 1.0 + Float(mantissa) / 1024.0)
                 }

                 let value = Double(f32)
                 if value > maxVal {
                     maxVal = value
                     maxIdx = i
                 }
             }
        default:
            print("Error in argmax: Unsupported MLMultiArray data type: \(dataType)")
            return -1
        }

        return maxIdx
    }

    // Optional: Function to display or plot results
     func displayResults() {
         // Access results via the @Published evaluationResults property in your View
         // This function can just log to console for now
         print("\n--- Evaluation Summary (Check UI for final results) ---")
         // Sort results by name before printing for consistency
         let sortedResults = evaluationResults.sorted { $0.key < $1.key }

         for (name, (accuracy, avgTime)) in sortedResults {
             let fileSize = getModelFileSize(name: name)
             let sizeString = fileSize > 0 ? "\(String(format: "%.2f", fileSize / (1024*1024))) MB" : "N/A"

             print("Model: \(name)")
             // Accuracy is now meaningful
             print("  Accuracy: \(String(format: "%.4f", accuracy))")
             print("  Avg. Inference Time: \(String(format: "%.6f", avgTime)) s")
             print("  Estimated Size: \(sizeString)")
             print("--------------")
         }
     }

     // Helper to get model file size
     func getModelFileSize(name: String) -> Double {
         guard let modelURL = Bundle.main.url(forResource: name, withExtension: "mlmodelc") else {
             print("Could not find \(name).mlmodelc to get size.")
             return 0
         }
         do {
             let attributes = try FileManager.default.attributesOfItem(atPath: modelURL.path)
             // Use optional binding for safety
             if let fileSize = attributes[.size] as? NSNumber {
                 return fileSize.doubleValue
             } else {
                 print("Could not retrieve file size attribute for \(name).")
                 return 0
             }
         } catch {
             print("Error getting file size for \(name): \(error)")
             return 0
         }
     }
}

// --- Helper for using MLMultiArray as input ---
// Core ML prediction often expects an MLFeatureProvider
class MLFeatureProviderWrapper: MLFeatureProvider {
    let featureProvider: MLMultiArray
    let inputName: String // Input feature name must match the model's expected input name.

    var featureNames: Set<String> {
        return [inputName]
    }

    func featureValue(for featureName: String) -> MLFeatureValue? {
        if featureName == inputName {
            // Ensure the MLMultiArray is provided in the expected format
            return MLFeatureValue(multiArray: featureProvider)
        }
        return nil
    }

    init(featureProvider: MLMultiArray, inputName: String) {
        self.featureProvider = featureProvider
        self.inputName = inputName // Set the correct input name
    }
}


// --- Example Usage View ---
struct ModelComparisonView: View {
    @StateObject private var evaluator = ModelEvaluator() // Use StateObject

    var body: some View {
        NavigationView { // Add NavigationView for title
            VStack {
                if evaluator.isEvaluating {
                    ProgressView("Evaluating models...")
                        .padding()
                }

                // Use the published evaluationResults
                List {
                     // Sort results for consistent display order
                    ForEach(evaluator.evaluationResults.sorted { $0.key < $1.key }, id: \.key) { key, value in
                        VStack(alignment: .leading) {
                            Text("Model: \(key)").font(.headline)
                            // Accuracy is now meaningful
                            Text("  Accuracy: \(String(format: "%.4f", value.0))")
                            Text("  Avg. Time: \(String(format: "%.6f", value.1))s")
                            Text("  Size: \(String(format: "%.2f", evaluator.getModelFileSize(name: key) / (1024*1024))) MB")
                        }
                        .padding(.vertical, 4)
                    }
                }
                .listStyle(InsetGroupedListStyle()) // Use a modern list style

                Spacer() // Pushes button to bottom

                Button("Run Evaluation") {
                    // Run evaluation in the background
                    DispatchQueue.global(qos: .userInitiated).async {
                        evaluator.evaluateAllModels()
                    }
                }
                .padding()
                .buttonStyle(.borderedProminent) // Make button more prominent
                .disabled(evaluator.isEvaluating) // Disable button during evaluation

            }
            .navigationTitle("Model Comparison") // Set a title
            .onAppear {
                 // Optionally run evaluation automatically on appear
                 // print("ModelComparisonView appeared. Kicking off evaluation.")
                 // DispatchQueue.global(qos: .userInitiated).async {
                 //     evaluator.evaluateAllModels()
                 // }
            }
        }
    }
}

// --- Preview Provider ---
struct ModelComparisonPreview: PreviewProvider {
    static var previews: some View {
        ModelComparisonView()
            // You can create a mock evaluator for previews if needed
            // .environmentObject(MockModelEvaluator())
            .preferredColorScheme(.dark)
    }
}
